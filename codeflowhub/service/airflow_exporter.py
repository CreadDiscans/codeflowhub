import os
import base64
import sys
import hashlib
from datetime import datetime, timedelta
from typing import Optional
from pathlib import Path

class AirflowExporter:
    """Airflow DAGë¡œ exportí•˜ëŠ” ì„œë¹„ìŠ¤"""

    def __init__(self, flow_decorator):
        self.flow = flow_decorator
        self.dag_id = (flow_decorator.flow_name or flow_decorator.name).replace('_', '-')

        # workflow íŒŒì¼ëª… ì¶”ì¶œ
        workflow_file_path = sys.modules[flow_decorator.func.__module__].__file__
        self.workflow_filename = os.path.basename(workflow_file_path)

        # ì„ íƒëœ í™˜ê²½ì˜ ì„¤ì • ì‚¬ìš©
        self.env = flow_decorator.env
        self.current_env = flow_decorator.current_env

        # K8s ì„¤ì •
        self.k8s_image = self.env.get('K8S_IMAGE', 'python:3.11-slim')
        self.k8s_namespace = self.env.get('K8S_NAMESPACE', 'airflow')

        # XCom sidecar image: flowì˜ airflow_sidecar_imageë¥¼ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ envì—ì„œ
        self.xcom_sidecar_image = flow_decorator.airflow_sidecar_image or self.env.get('XCOM_SIDECAR_IMAGE', None)

        # ì¶”ê°€ íŒ¨í‚¤ì§€ ê²½ë¡œë“¤ (envì—ì„œ ì„¤ì • ê°€ëŠ¥)
        self.extra_packages = self.env.get('EXTRA_PACKAGES', [])

        # ConfigMap ì„¤ì •
        self.configmap_name = f'{self.dag_id}-code'
        self.configmap_mount_path = '/flowhub/code'

        # Common K8s ì„¤ì •
        self.common_config = self.env.get('K8S_COMMON', {
            'namespace': self.k8s_namespace,
            'cmds': ['/bin/sh', '-c'],
            'do_xcom_push': True,
            'startup_timeout_seconds': 3600,
        })

        # ì»¤ìŠ¤í…€ CLI ì¸ì ê°€ì ¸ì˜¤ê¸° (FlowDecoratorì—ì„œ ì €ì¥ëœ ê°’)
        from ..flow import FlowDecorator
        self.custom_cli_args = FlowDecorator._custom_cli_args.copy()

        # Flowì˜ annotations, service_account_name, volumesê°€ ìˆìœ¼ë©´ common_configì— ì¶”ê°€
        if flow_decorator.annotations:
            self.common_config['annotations'] = flow_decorator.annotations
        if flow_decorator.service_account_name:
            self.common_config['service_account_name'] = flow_decorator.service_account_name
        if flow_decorator.volumes:
            # Volume ê°ì²´ë¥¼ k8s.V1Volume í˜•íƒœë¡œ ë³€í™˜
            volumes_list = []
            for vol in flow_decorator.volumes:
                if hasattr(vol, 'name') and hasattr(vol, 'persistent_volume_claim'):
                    volumes_list.append({
                        'name': vol.name,
                        'persistent_volume_claim': vol.persistent_volume_claim
                    })
            if volumes_list:
                self.common_config['volumes'] = volumes_list

    def export(self,
               output_path: str = None,
               schedule_interval: Optional[str] = None,
               start_date: Optional[datetime] = None,
               default_args: Optional[dict] = None):
        """Airflow DAG Python íŒŒì¼ë¡œ export"""
        if output_path is None:
            output_path = f'dags/{self.dag_id}_dag.py'

        if start_date is None:
            start_date = datetime(2024, 1, 1)

        if default_args is None:
            default_args = {
                'owner': 'flowhub',
                'start_date': start_date,
                'retries': 1,
                'retry_delay': timedelta(minutes=5),
            }

        # ë””ë ‰í† ë¦¬ ìƒì„±
        dir_path = os.path.dirname(output_path)
        if dir_path:
            os.makedirs(dir_path, exist_ok=True)

        # ConfigMap YAML ìƒì„± (repoê°€ ì—†ì„ ë•Œë§Œ)
        configmap_path = None
        if not self.flow.repo:
            configmap_path = self._generate_configmap_yaml(dir_path)

        # DAG ì½”ë“œ ìƒì„±
        dag_code = self._generate_dag_code(schedule_interval, start_date, default_args)

        # íŒŒì¼ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(dag_code)
            f.flush()
            os.fsync(f.fileno())

        print(f'âœ… Airflow DAG exported to: {output_path}')
        if configmap_path:
            print(f'âœ… ConfigMap YAML exported to: {configmap_path}')
            print(f'ğŸ“‹ Apply ConfigMap: kubectl apply -f {configmap_path}')
        return output_path

    def _generate_dag_code(self, schedule_interval, start_date, default_args):
        """DAG Python ì½”ë“œ ìƒì„± (target.py ìŠ¤íƒ€ì¼)"""

        flow_name = self.flow.flow_name or self.flow.name
        header = f'''"""
Airflow DAG for Workflow: {flow_name}
Auto-generated from FloWhub on {datetime.now().isoformat()}
"""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
from kubernetes.client import models as k8s
'''

        # XCom sidecar ì„¤ì •
        if self.xcom_sidecar_image:
            header += f'''
from airflow.providers.cncf.kubernetes.utils.xcom_sidecar import PodDefaults
PodDefaults.SIDECAR_CONTAINER.image = '{self.xcom_sidecar_image}'
'''

        # ê³µí†µ setup ìŠ¤í¬ë¦½íŠ¸
        if self.flow.repo:
            # repoê°€ ìˆìœ¼ë©´ git clone
            setup_script = f"git clone {self.flow.repo} /app\n                mkdir -p /app/input"
        else:
            # ConfigMapì—ì„œ ì½”ë“œ ë³µì‚¬ (base64 â†’ ë§ˆìš´íŠ¸ëœ íŒŒì¼ ì‚¬ìš©)
            setup_script = f"mkdir -p /app/input\n                cp {self.configmap_mount_path}/{self.workflow_filename} /app/{self.workflow_filename}"
            if self.extra_packages:
                for pkg_path in self.extra_packages:
                    if os.path.exists(pkg_path):
                        pkg_name = os.path.basename(pkg_path)
                        pkg_tar_name = f'{pkg_name}.tar.gz'
                        setup_script += f"\n                tar -xzf {self.configmap_mount_path}/{pkg_tar_name} -C /app"
                    else:
                        raise FileNotFoundError(
                            f"âŒ EXTRA_PACKAGESì— ì§€ì •ëœ íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pkg_path}\n"
                            f"   í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\n"
                            f"   ì ˆëŒ€ ê²½ë¡œë¡œ ì§€ì •í•˜ê±°ë‚˜, workflow íŒŒì¼ê³¼ ê°™ì€ ìœ„ì¹˜ì— íŒ¨í‚¤ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
                        )

        header += f'''# Common setup script
SETUP_SCRIPT = """{setup_script}"""

'''

        # DAG ì •ì˜
        default_args_str = self._format_dict(default_args, base_indent=1)
        description = self.flow.description or f'{flow_name} workflow'
        tags_str = repr(self.flow.tags) if self.flow.tags else f"['{self.flow.namespace}']"

        # Params ì„¤ì •
        params_line = ""
        if self.flow.params:
            params_str = self._format_dict(self.flow.params, indent=1)
            params_line = f"\n    params={params_str},"

        dag_definition = f'''dag = DAG(
    '{self.dag_id}',
    default_args={default_args_str},
    description='{description}',
    schedule_interval={repr(schedule_interval)},
    catchup=False,
    tags={tags_str},{params_line}
)

'''

        # Common ì„¤ì • - ConfigMap ë³¼ë¥¨/ë§ˆìš´íŠ¸ ì¶”ê°€
        if not self.flow.repo:
            # ConfigMap ë³¼ë¥¨/ë§ˆìš´íŠ¸ë¥¼ commonì— ì§ì ‘ í¬í•¨ (ê°’ì„ ì§ì ‘ ì¸ë¼ì¸)
            configmap_volume = f"k8s.V1Volume(\n            name='code-volume',\n            config_map=k8s.V1ConfigMapVolumeSource(name='{self.configmap_name}')\n        )"
            configmap_volume_mount = f"k8s.V1VolumeMount(\n            name='code-volume',\n            mount_path='{self.configmap_mount_path}',\n            read_only=True\n        )"

            common_config_copy = self.common_config.copy()

            # ê¸°ì¡´ volumesê°€ ìˆìœ¼ë©´ configmap_volume ì¶”ê°€, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            if 'volumes' in common_config_copy:
                existing_volumes_list = self._format_volumes_list(common_config_copy['volumes'])
                existing_volumes_list.append(configmap_volume)
                common_config_copy['volumes'] = '[\n        ' + ',\n        '.join(existing_volumes_list) + '\n    ]'
            else:
                common_config_copy['volumes'] = f'[\n        {configmap_volume}\n    ]'

            # volume_mountsëŠ” commonì— í¬í•¨í•˜ì§€ ì•ŠìŒ (taskë³„ë¡œ ë³‘í•©í•´ì•¼ í•˜ë¯€ë¡œ)
            # ëŒ€ì‹  base_volume_mounts ë³€ìˆ˜ë¡œ ë¶„ë¦¬
            common_config_str = self._format_dict_with_raw_values(common_config_copy, indent=0)
            common_definition = f'''common = {common_config_str}

# Base volume mounts (ConfigMap mount)
base_volume_mounts = [
    {configmap_volume_mount}
]

'''
        else:
            common_config_str = self._format_dict(self.common_config, indent=0)
            common_definition = f'''common = {common_config_str}

'''

        # Task ì •ì˜ë“¤ (ì¼ë°˜ workflow tasksë§Œ)
        tasks_code = "with dag:\n"
        for i, task in enumerate(self.flow.depend):
            task_code = self._generate_task_operator(task, is_first=(i == 0))
            tasks_code += task_code + "\n"

        # Failure handler task ì¶”ê°€ (trigger_rule='one_failed')
        if self.flow.on_failure:
            failure_task_code = self._generate_task_operator(
                self.flow.on_failure,
                is_first=True,
                trigger_rule='one_failed'
            )
            tasks_code += failure_task_code + "\n"

        # Task ì˜ì¡´ì„±
        dependencies = self._generate_dependencies()
        if dependencies:
            tasks_code += "    # Task dependencies\n"
            for dep in dependencies:
                tasks_code += f"    {dep}\n"

        # Failure handler ì˜ì¡´ì„± ì¶”ê°€
        if self.flow.on_failure:
            last_tasks = self._find_last_tasks()
            if last_tasks:
                tasks_code += f"    # Failure handler dependency\n"
                tasks_code += f"    [{', '.join(last_tasks)}] >> {self.flow.on_failure.name}\n"

        return header + dag_definition + common_definition + tasks_code

    def _generate_task_operator(self, task, is_first=False, trigger_rule=None):
        """KubernetesPodOperator ìƒì„±"""
        input_commands = self._generate_input_commands(task, is_first)

        # ì»¤ìŠ¤í…€ CLI ì¸ìë¥¼ ëª…ë ¹ì¤„ì— ì¶”ê°€
        custom_args_str = self._build_custom_cli_args()

        arguments = f'''
                {{SETUP_SCRIPT}}
                {input_commands}
                cd /app && python3 -u {self.workflow_filename} --env {self.current_env} --id {{{{{{{{ dag_run.run_id }}}}}}}} --task {task.name}{custom_args_str} --input /app/input --output /airflow/xcom/return.json
            '''

        task_image = task.image if hasattr(task, 'image') and task.image else self.k8s_image

        tolerations_code = self._build_tolerations_code(task)
        node_selector_code = self._build_node_selector_code(task)
        volume_mounts_code = self._build_volume_mounts_code(task)
        container_resources_code = self._build_container_resources_code(task)

        # trigger_rule ì¶”ê°€
        trigger_rule_code = f"\n        trigger_rule='{trigger_rule}'," if trigger_rule else ""

        # failure handlerëŠ” retries=0 ì„¤ì •
        retries_code = "\n        retries=0," if trigger_rule == 'one_failed' else ""

        operator_code = f'''    {task.name} = KubernetesPodOperator(
        **common,
        task_id='{task.name}',
        image='{task_image}',{trigger_rule_code}{retries_code}{tolerations_code}{node_selector_code}{volume_mounts_code}{container_resources_code}
        arguments=[
            f\'\'\'{arguments}\'\'\'
        ],
    )'''

        return operator_code

    def _generate_input_commands(self, task, is_first):
        """Input ë°ì´í„° ëª…ë ¹ì–´ ìƒì„±"""
        if is_first:
            return '\n'.join(["cat << 'EOF' > /app/input/0.json", '{{{{ params | tojson }}}}', 'EOF'])
        if not task.depend:
            return "echo '{{}}' >> /app/input/0.json"

        commands = [
            '\n'.join([f"cat << 'EOF' > /app/input/{i}.json", f'{{{{{{{{ ti.xcom_pull(task_ids=\"{dep.name}\") | tojson }}}}}}}}', 'EOF'])
            for i, dep in enumerate(task.depend)]
        return "\n                ".join(commands)

    def _build_tolerations_code(self, task):
        """Tolerations ì½”ë“œ ìƒì„±"""
        if not (hasattr(task, 'tolerations') and task.tolerations):
            return ""

        tolerations_items = []
        for tol in task.tolerations:
            tol_params = [f"{attr}='{getattr(tol, attr)}'"
                         for attr in ['key', 'operator', 'value', 'effect']
                         if hasattr(tol, attr) and getattr(tol, attr)]
            if tol_params:
                tolerations_items.append(f"k8s.V1Toleration({', '.join(tol_params)})")

        return f"\n        tolerations=[{', '.join(tolerations_items)}]," if tolerations_items else ""

    def _build_node_selector_code(self, task):
        """Node selector ì½”ë“œ ìƒì„±"""
        if hasattr(task, 'node_selector') and task.node_selector:
            return f"\n        node_selector={repr(task.node_selector)},"
        return ""

    def _build_volume_mounts_code(self, task, include_base=True):
        """Volume mounts ì½”ë“œ ìƒì„±

        Taskë³„ volume_mountsê°€ ìˆìœ¼ë©´ base_volume_mountsì™€ ë³‘í•©.
        ConfigMap ë§ˆìš´íŠ¸(code-volume)ëŠ” í•­ìƒ í¬í•¨ë˜ì–´ì•¼ í•¨.

        Args:
            task: TaskDecorator
            include_base: base_volume_mountsë¥¼ í¬í•¨í• ì§€ ì—¬ë¶€ (repoê°€ ì—†ì„ ë•Œ True)
        """
        volume_mounts_items = []
        if hasattr(task, 'volume_mounts') and task.volume_mounts:
            for vm in task.volume_mounts:
                vm_params = []
                if hasattr(vm, 'name') and vm.name:
                    vm_params.append(f"name='{vm.name}'")
                if hasattr(vm, 'mount_path') and vm.mount_path:
                    vm_params.append(f"mount_path='{vm.mount_path}'")
                if hasattr(vm, 'readOnly'):
                    vm_params.append(f"read_only={vm.readOnly}")
                if vm_params:
                    volume_mounts_items.append(f"k8s.V1VolumeMount({', '.join(vm_params)})")

        # ConfigMap ë§ˆìš´íŠ¸ì™€ ë³‘í•© (repoê°€ ì—†ì„ ë•Œ)
        if not self.flow.repo:
            if volume_mounts_items:
                return f"\n        volume_mounts=base_volume_mounts + [{', '.join(volume_mounts_items)}],"
            else:
                return f"\n        volume_mounts=base_volume_mounts,"

        # repoê°€ ìˆì„ ë•ŒëŠ” taskë³„ volume_mountsë§Œ ì‚¬ìš©
        return f"\n        volume_mounts=[{', '.join(volume_mounts_items)}]," if volume_mounts_items else ""

    def _build_custom_cli_args(self):
        """ì»¤ìŠ¤í…€ CLI ì¸ìë¥¼ ëª…ë ¹ì¤„ ë¬¸ìì—´ë¡œ ë³€í™˜

        ì˜ˆ: {'mode': 'prod', 'debug': True} â†’ ' --mode prod --debug'
        """
        if not self.custom_cli_args:
            return ""

        args_parts = []
        for key, value in self.custom_cli_args.items():
            # keyë¥¼ CLI í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (underscore â†’ dash)
            cli_key = key.replace('_', '-')

            if isinstance(value, bool):
                if value:
                    args_parts.append(f"--{cli_key}")
            else:
                args_parts.append(f"--{cli_key} {value}")

        return ' ' + ' '.join(args_parts) if args_parts else ""

    def _build_container_resources_code(self, task):
        """Container resources ì½”ë“œ ìƒì„±"""
        resource_attrs = ['request_cpu', 'request_memory', 'limit_cpu', 'limit_memory', 'limit_gpu']
        if not any(hasattr(task, attr) for attr in resource_attrs):
            return ""

        requests = {}
        limits = {}
        if hasattr(task, 'request_cpu') and task.request_cpu:
            requests['cpu'] = task.request_cpu
        if hasattr(task, 'request_memory') and task.request_memory:
            requests['memory'] = task.request_memory
        if hasattr(task, 'limit_cpu') and task.limit_cpu:
            limits['cpu'] = task.limit_cpu
        if hasattr(task, 'limit_memory') and task.limit_memory:
            limits['memory'] = task.limit_memory
        if hasattr(task, 'limit_gpu') and task.limit_gpu:
            limits['nvidia.com/gpu'] = task.limit_gpu

        resource_requirements = f"k8s.V1ResourceRequirements(requests={repr(requests)}, limits={repr(limits)})"
        return f"\n        container_resources={resource_requirements},"

    def _encode_workflow(self):
        """ì „ì²´ workflow.py ì½”ë“œë¥¼ base64ë¡œ ì¸ì½”ë”©"""
        import sys
        workflow_file = sys.modules[self.flow.func.__module__].__file__
        with open(workflow_file, 'r', encoding='utf-8') as f:
            workflow_code = f.read()
        return base64.b64encode(workflow_code.encode()).decode()

    def _encode_package(self, package_path):
        """íŒ¨í‚¤ì§€ë¥¼ tar.gzë¡œ ì••ì¶• í›„ base64 ì¸ì½”ë”© (__pycache__ ë“± ì œì™¸)"""
        import tarfile
        import io

        exclude_patterns = {
            '__pycache__', '.pyc', '.pyo', '.pyd', '.so', '.git', '.gitignore',
            '.pytest_cache', '.mypy_cache', '.coverage', '.DS_Store', '.egg-info', 'dist', 'build'
        }
        exclude_extensions = ('.pyc', '.pyo', '.pyd', '.so')

        def should_exclude(name):
            """íŒŒì¼/ë””ë ‰í† ë¦¬ë¥¼ ì œì™¸í• ì§€ íŒë‹¨"""
            parts = Path(name).parts
            return any(part in exclude_patterns or part.endswith(exclude_extensions) for part in parts)

        tar_buffer = io.BytesIO()
        with tarfile.open(fileobj=tar_buffer, mode='w:gz') as tar:
            package_name = os.path.basename(package_path)
            for root, dirs, files in os.walk(package_path):
                dirs[:] = [d for d in dirs if not should_exclude(os.path.join(root, d))]
                for file in files:
                    file_path = os.path.join(root, file)
                    if not should_exclude(file_path):
                        arcname = os.path.join(package_name, os.path.relpath(file_path, package_path))
                        tar.add(file_path, arcname=arcname)

        tar_buffer.seek(0)
        compressed_size = len(tar_buffer.getvalue())
        encoded = base64.b64encode(tar_buffer.read()).decode()
        print(f'ğŸ“¦ Package compressed: {compressed_size/1024:.2f} KB â†’ Base64: {len(encoded)/1024:.2f} KB')
        return encoded

    def _find_last_tasks(self):
        """ë§ˆì§€ë§‰ taskë“¤ (ë‹¤ë¥¸ taskì˜ dependencyê°€ ì•„ë‹Œ taskë“¤) ì°¾ê¸°"""
        # ëª¨ë“  ì¼ë°˜ task (flow.dependëŠ” on_failureë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŒ)
        all_tasks = {task.name for task in self.flow.depend}

        # dependencyë¡œ ì‚¬ìš©ë˜ëŠ” taskë“¤ ìˆ˜ì§‘
        dependency_tasks = {
            dep.name
            for task in self.flow.depend
            if task.depend
            for dep in task.depend
        }

        # ë§ˆì§€ë§‰ taskë“¤ = ì „ì²´ - dependencyë¡œ ì‚¬ìš©ë˜ëŠ” ê²ƒë“¤
        return sorted(all_tasks - dependency_tasks)

    def _generate_dependencies(self):
        """Task ì˜ì¡´ì„± ìƒì„±"""
        dependencies = []
        for task in self.flow.depend:
            if task.depend:
                dep_names = [dep.name for dep in task.depend]
                if len(dep_names) == 1:
                    dependencies.append(f"{dep_names[0]} >> {task.name}")
                else:
                    deps_str = ', '.join(dep_names)
                    dependencies.append(f"[{deps_str}] >> {task.name}")
        return dependencies

    def _format_dict(self, d, indent=0, base_indent=0):
        """Dictë¥¼ Python ì½”ë“œ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…"""
        return self._format_dict_with_raw_values(d, indent, raw_keys=set(), base_indent=base_indent)

    def _format_dict_with_raw_values(self, d, indent=0, raw_keys=None, base_indent=0):
        """Dictë¥¼ Python ì½”ë“œ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…

        Args:
            d: í¬ë§·íŒ…í•  dict
            indent: ë“¤ì—¬ì“°ê¸° ë ˆë²¨
            raw_keys: ê°’ì„ ë¬¸ìì—´ë¡œ ê°ì‹¸ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì¶œë ¥í•  í‚¤ë“¤ (volumes, volume_mounts ë“±)
            base_indent: ê¸°ë³¸ ë“¤ì—¬ì“°ê¸° (DAG ì •ì˜ ë‚´ë¶€ì—ì„œ ì‚¬ìš© ì‹œ)
        """
        if not d:
            return "{}"

        if raw_keys is None:
            raw_keys = {'volumes', 'volume_mounts'}

        items = []
        for key, value in d.items():
            # raw_keysì— í•´ë‹¹í•˜ëŠ” í‚¤ëŠ” ê°’ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥ (ì´ë¯¸ ì½”ë“œ ë¬¸ìì—´ì¸ ê²½ìš°)
            if key in raw_keys and isinstance(value, str):
                items.append(f"'{key}': {value}")
            elif isinstance(value, str):
                items.append(f"'{key}': '{value}'")
            elif isinstance(value, datetime):
                items.append(f"'{key}': datetime({value.year}, {value.month}, {value.day})")
            elif isinstance(value, timedelta):
                items.append(f"'{key}': timedelta(minutes={int(value.total_seconds() / 60)})")
            elif isinstance(value, bool):
                items.append(f"'{key}': {value}")
            elif isinstance(value, (int, float)):
                items.append(f"'{key}': {value}")
            elif isinstance(value, dict):
                items.append(f"'{key}': {self._format_dict_with_raw_values(value, indent + 1, raw_keys, base_indent)}")
            elif isinstance(value, list):
                # volumes ë¦¬ìŠ¤íŠ¸ë¥¼ íŠ¹ë³„ ì²˜ë¦¬
                if key == 'volumes':
                    volumes_str = self._format_volumes(value)
                    items.append(f"'{key}': {volumes_str}")
                else:
                    items.append(f"'{key}': {value}")
            else:
                items.append(f"'{key}': {repr(value)}")

        total_indent = base_indent + indent
        indent_str = '    ' * total_indent
        if len(items) <= 2:
            return '{' + ', '.join(items) + '}'
        else:
            return '{\n' + indent_str + '    ' + f',\n{indent_str}    '.join(items) + '\n' + indent_str + '}'

    def _format_volumes(self, volumes_list):
        """Volumes ë¦¬ìŠ¤íŠ¸ë¥¼ k8s.V1Volume í˜•íƒœë¡œ í¬ë§·íŒ… (í•œ ì¤„)"""
        volumes_items = []
        for vol in volumes_list:
            if isinstance(vol, dict) and 'name' in vol and 'persistent_volume_claim' in vol:
                pvc_claim = f"k8s.V1PersistentVolumeClaimVolumeSource(claim_name='{vol['persistent_volume_claim']}')"
                volumes_items.append(f"k8s.V1Volume(name='{vol['name']}', persistent_volume_claim={pvc_claim})")

        if len(volumes_items) == 0:
            return "[]"
        elif len(volumes_items) == 1:
            return f"[{volumes_items[0]}]"
        else:
            return "[\n        " + ",\n        ".join(volumes_items) + "\n    ]"

    def _format_volumes_list(self, volumes_list):
        """Volumes ë¦¬ìŠ¤íŠ¸ë¥¼ k8s.V1Volume ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ (ì—¬ëŸ¬ ì¤„ í¬ë§·)"""
        volumes_items = []
        for vol in volumes_list:
            if isinstance(vol, dict) and 'name' in vol and 'persistent_volume_claim' in vol:
                volumes_items.append(f"""k8s.V1Volume(
            name='{vol['name']}',
            persistent_volume_claim=k8s.V1PersistentVolumeClaimVolumeSource(
                claim_name='{vol['persistent_volume_claim']}'
            )
        )""")
        return volumes_items

    def _generate_configmap_yaml(self, output_dir):
        """ConfigMap YAML íŒŒì¼ ìƒì„±

        workflow ì½”ë“œì™€ ì¶”ê°€ íŒ¨í‚¤ì§€ë¥¼ ConfigMapì˜ binaryDataë¡œ ì €ì¥.
        Kubernetes ConfigMapì˜ ìµœëŒ€ í¬ê¸°ëŠ” 1MBì´ë¯€ë¡œ, í•„ìš”ì‹œ ì—¬ëŸ¬ ConfigMapìœ¼ë¡œ ë¶„í• .
        """
        import yaml

        configmap_path = os.path.join(output_dir, f'{self.configmap_name}.yaml')

        # workflow ì½”ë“œ ì½ê¸°
        workflow_file = sys.modules[self.flow.func.__module__].__file__
        with open(workflow_file, 'r', encoding='utf-8') as f:
            workflow_code = f.read()

        # ConfigMap ë°ì´í„° ì¤€ë¹„
        data = {
            self.workflow_filename: workflow_code
        }

        binary_data = {}

        # ì¶”ê°€ íŒ¨í‚¤ì§€ë“¤ì„ tar.gzë¡œ ì••ì¶•í•˜ì—¬ binaryDataì— ì¶”ê°€
        if self.extra_packages:
            for pkg_path in self.extra_packages:
                if not os.path.exists(pkg_path):
                    raise FileNotFoundError(
                        f"âŒ EXTRA_PACKAGESì— ì§€ì •ëœ íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pkg_path}\n"
                        f"   í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\n"
                        f"   ì ˆëŒ€ ê²½ë¡œë¡œ ì§€ì •í•˜ê±°ë‚˜, workflow íŒŒì¼ê³¼ ê°™ì€ ìœ„ì¹˜ì— íŒ¨í‚¤ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
                    )
                pkg_name = os.path.basename(pkg_path)
                pkg_tar_name = f'{pkg_name}.tar.gz'
                pkg_b64 = self._encode_package(pkg_path)
                binary_data[pkg_tar_name] = pkg_b64

        # ConfigMap YAML ìƒì„±
        configmap = {
            'apiVersion': 'v1',
            'kind': 'ConfigMap',
            'metadata': {
                'name': self.configmap_name,
                'namespace': self.k8s_namespace,
                'labels': {
                    'app': self.dag_id,
                    'managed-by': 'flowhub'
                }
            },
            'data': data
        }

        if binary_data:
            configmap['binaryData'] = binary_data

        # YAML íŒŒì¼ ì €ì¥
        with open(configmap_path, 'w', encoding='utf-8') as f:
            yaml.dump(configmap, f, default_flow_style=False, allow_unicode=True)

        # ConfigMap í¬ê¸° ì²´í¬ (ê²½ê³ )
        total_size = len(workflow_code)
        for key, value in binary_data.items():
            total_size += len(value)

        if total_size > 1024 * 1024:  # 1MB
            print(f'âš ï¸  Warning: ConfigMap size ({total_size / 1024:.1f}KB) exceeds 1MB limit.')
            print(f'   Consider using git repo option or splitting into multiple ConfigMaps.')

        return configmap_path
