import argparse
import inspect
import json
import os
import sys
from pathlib import Path

from .base import BaseDecorator
from .action import Action
from .service import AirflowExporter

class FlowDecorator(BaseDecorator):

    namespace:str
    env:dict
    env_config:dict  # ì—¬ëŸ¬ í™˜ê²½ ì„¤ì •ì„ ë‹´ëŠ” dict
    current_env:str  # í˜„ì¬ ì„ íƒëœ í™˜ê²½
    flow_name:str  # flowì˜ ì´ë¦„ (name íŒŒë¼ë¯¸í„°)
    description:str  # flowì˜ ì„¤ëª…
    params:dict  # Airflow DAG params
    tags:list  # Airflow DAG tags
    annotations:dict  # Kubernetes pod annotations
    service_account_name:str  # Kubernetes service account name
    volumes:list  # Kubernetes volumes
    airflow_sidecar_image:str  # Airflow XCom sidecar ì´ë¯¸ì§€
    repo:str  # Git repository URL
    on_failure: 'BaseDecorator' = None  # ëª¨ë“  taskì˜ ê¸°ë³¸ failure handler

    def __init__(self, *args, namespace='default', env=None, name=None, description=None, params=None,
                 tags=None, annotations=None, service_account_name=None, volumes=None,
                 airflow_sidecar_image=None, repo=None, on_failure=None, **kwargs):
        # CLI ì†ì„± ë¨¼ì € ì´ˆê¸°í™” (init()ì—ì„œ ì‚¬ìš©ë¨)
        self._cli_export = None
        self._cli_job_dir = None
        self._cli_task = None
        self._cli_input_data = None
        self._cli_output_file = None
        self._cli_run_log = None

        self.flow_name = name
        self.namespace = namespace
        self.description = description
        self.params = params or {}
        self.tags = tags or []
        self.annotations = annotations or {}
        self.service_account_name = service_account_name
        self.volumes = volumes or []
        self.airflow_sidecar_image = airflow_sidecar_image
        self.repo = repo
        self.on_failure = on_failure

        self._initialize_env(env)
        self._parse_args()

        # super().__init__()ëŠ” ë§ˆì§€ë§‰ì— í˜¸ì¶œ (init()ì´ ì‹¤í–‰ë¨)
        super().__init__(*args, name=name, **kwargs)

    def _initialize_env(self, env):
        """í™˜ê²½ ì„¤ì • ì´ˆê¸°í™”"""
        if env and isinstance(env, dict) and all(isinstance(v, dict) for v in env.values()):
            self.env_config = env
            self.env = env.get('default', {})
        elif env:
            self.env_config = {'default': env}
            self.env = env
        else:
            self.env_config = {}
            self.env = {}
        self.current_env = 'default'

    # í´ë˜ìŠ¤ ë³€ìˆ˜: ë¯¸ë¦¬ íŒŒì‹±ëœ ê²°ê³¼ ì €ì¥
    _pre_parsed_args = None
    # í´ë˜ìŠ¤ ë³€ìˆ˜: ì»¤ìŠ¤í…€ CLI ì¸ì ì €ì¥ (flowhub ê¸°ë³¸ ì¸ì ì œì™¸)
    _custom_cli_args = {}

    @classmethod
    def _create_parser(cls, add_help=True):
        """FlowhHub CLI íŒŒì„œ ìƒì„±"""
        parser = argparse.ArgumentParser(description='FlowhHub Workflow', add_help=add_help)
        parser.add_argument('--env', type=str, default='default', help='Environment to use (default: default)')
        parser.add_argument('--id', type=str, default=None, help='Workflow run ID (default: run)')
        parser.add_argument('--export', type=str, default=None, help='Export to external system (airflow)')
        parser.add_argument('--job', type=str, default=None, help='Job directory path (auto-sets --input-data and --run-log)')
        parser.add_argument('--task', type=str, default=None, help='Run specific task (default: all when --job is used)')
        parser.add_argument('--input', '--input-data', type=str, default=None, dest='input_data', help='Input file or directory for task initial data')
        parser.add_argument('--output', '--output-file', type=str, default=None, dest='output_file', help='Output file path for final task result')
        parser.add_argument('--run-log', type=str, default='run.json', help='Run log file storing intermediate task results (default: run.json)')
        return parser

    @classmethod
    def get_parser(cls) -> argparse.ArgumentParser:
        """FlowhHub CLI íŒŒì„œë¥¼ ë°˜í™˜

        ì™¸ë¶€ì—ì„œ ì»¤ìŠ¤í…€ ì¸ìë¥¼ ì¶”ê°€í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        íŒŒì„œì— ì§ì ‘ add_argument()ë¡œ ì»¤ìŠ¤í…€ ì¸ìë¥¼ ì¶”ê°€í•œ í›„ parse_args()ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.

        Returns:
            argparse.ArgumentParser: FlowhHub ì¸ìê°€ í¬í•¨ëœ íŒŒì„œ

        Example:
            from flowhub import get_parser, parse_args, flow, task

            # 1. íŒŒì„œ ê°€ì ¸ì™€ì„œ ì»¤ìŠ¤í…€ ì¸ì ì¶”ê°€
            parser = get_parser()
            parser.add_argument('--my-option', type=str, default='default')
            parser.add_argument('--count', type=int, default=1)
            args = parse_args(parser)

            # 2. ì»¤ìŠ¤í…€ ê°’ì„ ì‚¬ìš©í•˜ì—¬ flow ì •ì˜
            @flow(name='my-workflow', env={
                'default': {
                    'WORKSPACE': './tmp',
                    'MY_OPTION': args.my_option,
                    'COUNT': args.count
                }
            })
            def main(flow_args):
                ...
        """
        return cls._create_parser()

    @classmethod
    def parse_args(cls, parser: argparse.ArgumentParser = None) -> argparse.Namespace:
        """CLI ì¸ìë¥¼ íŒŒì‹±í•˜ê³  ê²°ê³¼ë¥¼ ìºì‹œ

        get_parser()ë¡œ ì»¤ìŠ¤í…€ ì¸ìë¥¼ ì¶”ê°€í•œ íŒŒì„œë¥¼ ì „ë‹¬í•˜ê±°ë‚˜,
        íŒŒì„œ ì—†ì´ í˜¸ì¶œí•˜ë©´ ê¸°ë³¸ FlowhHub íŒŒì„œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

        Args:
            parser: ì»¤ìŠ¤í…€ ì¸ìê°€ ì¶”ê°€ëœ íŒŒì„œ (optional)

        Returns:
            argparse.Namespace: íŒŒì‹±ëœ ì¸ìë“¤

        Example:
            from flowhub import get_parser, parse_args, flow

            parser = get_parser()
            parser.add_argument('--my-option', type=str)
            args = parse_args(parser)

            @flow(name='my-workflow', env={'default': {'MY_OPTION': args.my_option}})
            def main(flow_args):
                ...
        """
        if parser is None:
            parser = cls._create_parser()

        if len(sys.argv) <= 1:
            cls._pre_parsed_args = parser.parse_args([])
            return cls._pre_parsed_args

        args = parser.parse_args()
        cls._pre_parsed_args = args

        # flowhub ê¸°ë³¸ ì¸ì ëª©ë¡
        flowhub_args = {'env', 'id', 'export', 'job', 'task', 'input_data', 'output_file', 'run_log'}

        # ì»¤ìŠ¤í…€ ì¸ì ì¶”ì¶œ (flowhub ê¸°ë³¸ ì¸ì ì œì™¸)
        cls._custom_cli_args = {
            key: value for key, value in vars(args).items()
            if key not in flowhub_args and value is not None
        }

        return args

    def _parse_args(self):
        """Command line arguments íŒŒì‹±

        parse_args()ë¡œ ë¯¸ë¦¬ íŒŒì‹±ëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©í•˜ê³ ,
        ì—†ìœ¼ë©´ parse_known_args()ë¡œ flowhub ì¸ìë§Œ íŒŒì‹±í•©ë‹ˆë‹¤.
        """
        # ë¯¸ë¦¬ íŒŒì‹±ëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if FlowDecorator._pre_parsed_args is not None:
            args = FlowDecorator._pre_parsed_args
        elif len(sys.argv) <= 1:
            self._cli_export = self._cli_task = self._cli_input_data = self._cli_output_file = self._cli_run_log = self._cli_job_dir = None
            return
        else:
            # parse_known_args()ë¡œ flowhub ì¸ìë§Œ íŒŒì‹±
            # ë‚´ë¶€ íŒŒì‹± ì‹œì—ëŠ” add_help=False (ì™¸ë¶€ íŒŒì„œì™€ ì¶©ëŒ ë°©ì§€)
            parser = self._create_parser(add_help=False)
            args, _ = parser.parse_known_args()

        if args.env and args.env in self.env_config:
            self.select_env(args.env)
            print(f'Using environment: {args.env}')

        if args.id:
            self.env['run_id'] = args.id
            print(f'Using run_id: {args.id}')

        # --job ì˜µì…˜ ì²˜ë¦¬
        if args.job:
            job_path = Path(args.job)
            self._cli_job_dir = args.job
            # --taskê°€ ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ 'all' ì‚¬ìš©
            self._cli_task = args.task if args.task else 'all'

            # --input-dataê°€ ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ job ê²½ë¡œ ê¸°ë°˜
            # (allì¸ ê²½ìš° input.json, íŠ¹ì • taskì¸ ê²½ìš° None=run-logì—ì„œ ë¡œë“œ)
            if not args.input_data:
                self._cli_input_data = str(job_path / 'input.json') if self._cli_task == 'all' else None
            else:
                self._cli_input_data = args.input_data

            # --run-logê°€ ê¸°ë³¸ê°’ì´ë©´ {job_path}/run.json ì‚¬ìš©
            self._cli_run_log = args.run_log if args.run_log != 'run.json' else str(job_path / 'run.json')
            self._cli_output_file = args.output_file

            print(f'ğŸ“ Job directory: {args.job}')
            print(f'   Task: {self._cli_task}')
            print(f'   Input data: {self._cli_input_data or f"(from {self._cli_run_log})"}')
            print(f'   Run log: {self._cli_run_log}')
        else:
            self._cli_task = args.task
            self._cli_input_data = args.input_data
            self._cli_output_file = args.output_file
            self._cli_run_log = args.run_log
            self._cli_job_dir = None

        self._cli_export = args.export

        # run log íŒŒì¼ ê²½ë¡œë¥¼ BaseDecorator.run_log_fileì— ì„¤ì •
        if self._cli_run_log:
            BaseDecorator.run_log_file = self._cli_run_log

    def select_env(self, env_name):
        """í™˜ê²½ ì„ íƒ"""
        if env_name not in self.env_config:
            available = list(self.env_config.keys())
            raise ValueError(f'í™˜ê²½ "{env_name}"ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ í™˜ê²½: {available}')

        self.current_env = env_name
        self.env = self.env_config[env_name]
        return self

    def _handle_export(self):
        """Export ì²˜ë¦¬"""
        if self._cli_export.lower() != 'airflow':
            print(f'âŒ Unknown export target: {self._cli_export}')
            print(f'   Supported: airflow')
            os._exit(1)

        from datetime import datetime, timedelta

        flow_id = self.flow_name or self.name
        print(f'ğŸš€ Exporting {flow_id} to Airflow DAG...')

        dag_path = self.export_airflow(
            output_path=f'dags/{flow_id}_dag.py',
            schedule_interval=None,
            start_date=datetime.now(),
            default_args={
                'owner': 'flowhub',
                'retries': 1,
                'retry_delay': timedelta(minutes=5),
            }
        )

        print(f'âœ… Successfully exported to: {dag_path}')
        print(f'ğŸ“‹ DAG ID: {flow_id}')
        print(f'ğŸ·ï¸  Tags: [{self.namespace}]')
        os._exit(0)

    def _handle_task_run(self):
        """CLIì—ì„œ --task ì˜µì…˜ìœ¼ë¡œ íŠ¹ì • task ì‹¤í–‰"""
        # --task allì´ë©´ ëª¨ë“  taskë¥¼ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰
        if self._cli_task == 'all':
            self._run_all_tasks()
            return

        # ì…ë ¥ ë¡œë“œ
        input_data = self._load_input_data_from_cli(allow_none=True)

        # run() ë©”ì„œë“œ í˜¸ì¶œ
        result = self.run(self._cli_task, input_data)

        # ê²°ê³¼ ì €ì¥
        self._save_result_to_output(result, success_message=f'âœ… Task "{self._cli_task}" ì‹¤í–‰ ì™„ë£Œ')

    def _load_input_data_from_cli(self, allow_none=False):
        """CLI --input-data ì˜µì…˜ì—ì„œ ì…ë ¥ ë°ì´í„° ë¡œë“œ

        Args:
            allow_none: Trueë©´ --input-data ì—†ì„ ë•Œ None ë°˜í™˜, Falseë©´ ì—ëŸ¬
        """
        if not self._cli_input_data:
            if allow_none:
                return None
            else:
                print(f'âŒ Error: --input-dataê°€ í•„ìš”í•©ë‹ˆë‹¤.')
                os._exit(1)

        input_path = Path(self._cli_input_data)

        if input_path.is_dir():
            return self._load_inputs_from_dir(input_path)
        elif input_path.is_file():
            return self._load_input_from_file(input_path)
        else:
            print(f'âŒ Error: ì…ë ¥ ê²½ë¡œ {self._cli_input_data}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
            os._exit(1)

    def _load_inputs_from_dir(self, input_path):
        """ë””ë ‰í† ë¦¬ì—ì„œ ì…ë ¥ ë°ì´í„° ë¡œë“œ"""
        print(f'ğŸ“‚ Loading inputs from directory: {input_path}')
        json_files = sorted(input_path.glob('*.json'))

        if not json_files:
            print(f'âŒ Error: {input_path} í´ë”ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.')
            os._exit(1)

        input_data = []
        for json_file in json_files:
            print(f'   - {json_file.name}')
            with open(json_file, 'r', encoding='utf-8') as f:
                input_data.append(json.load(f))

        print(f'âœ… Loaded {len(input_data)} input file(s)')
        return input_data

    def _load_input_from_file(self, input_path):
        """íŒŒì¼ì—ì„œ ì…ë ¥ ë°ì´í„° ë¡œë“œ"""
        print(f'ğŸ“„ Loading input from file: {input_path}')
        with open(input_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _run_all_tasks(self):
        """ëª¨ë“  taskë¥¼ flow ì „ì²´ ì‹¤í–‰"""
        # ì…ë ¥ ë°ì´í„° ë¡œë“œ (í•„ìˆ˜)
        input_data = self._load_input_data_from_cli(allow_none=False)

        print(f'ğŸš€ Running full workflow...')
        print(f'ğŸ“‹ Tasks: {[t.name for t in self.depend]}')
        print()

        # flow ì „ì²´ ì‹¤í–‰ (self(data))
        result = self(input_data)

        # ê²°ê³¼ ì €ì¥
        self._save_result_to_output(result, success_message='\nâœ… Workflow completed')

    def _save_single_task_to_log(self, task_name, result):
        """ê°œë³„ task ê²°ê³¼ë¥¼ run.jsonì— ì €ì¥"""
        run_log = self._load_log_file()
        run_log[task_name] = {
            'input': result,
            'output': result
        }
        self._write_log_file(run_log)

    def _save_result_to_output(self, result, success_message='âœ… Task completed'):
        """ê²°ê³¼ë¥¼ --output-file ë˜ëŠ” run-logì— ì €ì¥

        Args:
            result: ì €ì¥í•  ê²°ê³¼
            success_message: ì„±ê³µ ë©”ì‹œì§€ (ê¸°ë³¸: 'âœ… Task completed')
        """
        if self._cli_output_file:
            # --output-file ì§€ì •ëœ ê²½ìš°: íŒŒì¼ì— ì €ì¥
            output_path = Path(self._cli_output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            print(f'{success_message}. Output file: {self._cli_output_file}')
        else:
            # --output-file ì—†ëŠ” ê²½ìš°: run-logì— ì €ì¥
            if self._cli_task and self._cli_task != 'all':
                self._save_single_task_to_log(self._cli_task, result)
            print(f'{success_message}. Results saved to run log: {BaseDecorator.run_log_file}')

    def _inject_env_to_data(self, data):
        """ë°ì´í„°ì— env ì£¼ì… (ì¤‘ë³µ ì½”ë“œ ë°©ì§€ìš© í—¬í¼ ë©”ì„œë“œ)"""
        if isinstance(data, dict):
            data = data.copy()
            if self.env is not None and 'env' not in data:
                env_with_defaults = self.env.copy()
                env_with_defaults.setdefault('project', self.flow_name or self.name)
                env_with_defaults.setdefault('run_id', 'run')
                env_with_defaults.setdefault('task', None)
                data['env'] = env_with_defaults
        return data

    def _wrapper_func(self, *args, **kwargs):
        """Flow ì‹¤í–‰ ì‹œ argsì— envë¥¼ ìë™ìœ¼ë¡œ ì£¼ì…"""
        if args and isinstance(args[0], dict):
            input_data = self._inject_env_to_data(args[0])
            args = (input_data,) + args[1:]

        try:
            return super()._wrapper_func(*args, **kwargs)
        except Exception as e:
            if self.on_failure:
                import traceback
                print(f'âŒ Workflow "{self.name}" failed: {e}')
                print(f'ğŸ”§ Calling failure handler: {self.on_failure.name}')
                # failure handlerì— env ì •ë³´ + ì—ëŸ¬ ì •ë³´ í¬í•¨ëœ ë°ì´í„° ì „ë‹¬
                failure_data = self._inject_env_to_data({
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'traceback': traceback.format_exc(),
                })
                return self.on_failure(failure_data)
            raise

    def init(self):
        # Actionì„ ì „ë‹¬í•˜ì—¬ flow ì‹¤í–‰ â†’ taskë“¤ì´ ìë™ìœ¼ë¡œ ì˜ì¡´ì„± ìˆ˜ì§‘
        action = Action(type='build')

        # Actionì— í˜¸ì¶œëœ taskë“¤ì„ ì¶”ì í•  ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
        action.called_tasks = []

        self.func(action)

        # flowì˜ dependì— ì‹¤ì œë¡œ í˜¸ì¶œëœ taskë¥¼ ì‹¤í–‰ ìˆœì„œë¡œ ì •ë ¬ (ë³‘ë ¬ì¸ ê²½ìš° ì´ë¦„ìˆœ)
        self.depend = self._sort_tasks_by_execution_order(action.called_tasks)

        # init ì™„ë£Œ í›„ CLI ëª¨ë“œ ì²´í¬
        if self._cli_export:
            self._handle_export()
        elif self._cli_task:
            self._handle_task_run()
            os._exit(0)

    def _sort_tasks_by_execution_order(self, tasks):
        """
        Taskë“¤ì„ ì‹¤í–‰ ìˆœì„œë¡œ ì •ë ¬ (topological sort)
        ê°™ì€ ë ˆë²¨(ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥)ì¸ taskë“¤ì€ ì´ë¦„ìˆœìœ¼ë¡œ ì •ë ¬
        """
        # ê° taskì˜ ë ˆë²¨(depth) ê³„ì‚°
        def get_task_level(task):
            if not task.depend:
                return 0
            return max(get_task_level(dep) for dep in task.depend) + 1

        # taskë¥¼ (level, name, task) íŠœí”Œë¡œ ë³€í™˜í•˜ì—¬ ì •ë ¬
        task_with_level = [(get_task_level(task), task.name, task) for task in tasks]
        task_with_level.sort(key=lambda x: (x[0], x[1]))  # level ìš°ì„ , ê°™ìœ¼ë©´ name ìˆœ

        return [task for _, _, task in task_with_level]

    def run(self, task_name, data=None):
        """íŠ¹ì • taskë§Œ ì‹¤í–‰

        Args:
            task_name: ì‹¤í–‰í•  task ì´ë¦„
            data: ì…ë ¥ ë°ì´í„°. dict ë˜ëŠ” list[dict] ê°€ëŠ¥
                  - None: run.jsonì—ì„œ ë¡œë“œ
                  - dict: ë‹¨ì¼ ì…ë ¥
                  - list[dict]: ì—¬ëŸ¬ ì…ë ¥ (task íŒŒë¼ë¯¸í„°ì— ë”°ë¼ ë³‘í•© ë˜ëŠ” ê°œë³„ ì „ë‹¬)
        """
        # ëª¨ë“  ì‹¤í–‰ ê°€ëŠ¥í•œ task ëª©ë¡ ìƒì„±
        all_tasks = list(self.depend)
        if self.on_failure:
            all_tasks.append(self.on_failure)

        # task ê²€ìƒ‰
        target_task = next((task for task in all_tasks if task.name == task_name), None)

        if not target_task:
            available_tasks = [t.name for t in all_tasks]
            raise ValueError(f'Task "{task_name}"ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ task: {available_tasks}')

        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
        if data is None:
            input_data = self._load_task_input_from_log(task_name)
            print(f'ğŸš€ Running task: {task_name} (from run.json)')
        else:
            input_data = data
            print(f'ğŸš€ Running task: {task_name}')

        # list[dict]ì¸ ê²½ìš° ì²˜ë¦¬
        if isinstance(input_data, list):
            sig = inspect.signature(target_task.func)
            param_count = len([p for p in sig.parameters.values() if p.default == inspect.Parameter.empty])

            if len(input_data) == 1:
                # ë‹¨ì¼ ì…ë ¥
                input_data = self._inject_env_to_data(input_data[0])
                result = target_task(input_data)
            elif param_count > 1:
                # ì—¬ëŸ¬ íŒŒë¼ë¯¸í„°ë¥¼ ë°›ëŠ” task - ê°œë³„ ì „ë‹¬
                input_data = [self._inject_env_to_data(d) for d in input_data]
                result = target_task(*input_data)
            else:
                # ë‹¨ì¼ íŒŒë¼ë¯¸í„° - ë³‘í•©
                merged = {}
                for d in input_data:
                    merged.update(d)
                merged = self._inject_env_to_data(merged)
                result = target_task(merged)
        else:
            # dictì¸ ê²½ìš°
            input_data = self._inject_env_to_data(input_data)
            result = target_task(input_data)

        # ê²°ê³¼ ì¶œë ¥
        print(f'ğŸ“¤ Output: {json.dumps(result, ensure_ascii=False, indent=2)}')

        return result

    def _load_task_input_from_log(self, task_name):
        """run.jsonì—ì„œ task input ë¡œë“œ

        - taskê°€ ì´ë¯¸ ì‹¤í–‰ëœ ì ì´ ìˆìœ¼ë©´: í•´ë‹¹ taskì˜ input ë°˜í™˜
        - taskê°€ ì‹¤í–‰ëœ ì ì´ ì—†ìœ¼ë©´: ì˜ì¡´í•˜ëŠ” taskë“¤ì˜ outputì„ ë³‘í•©í•˜ì—¬ ë°˜í™˜
        """
        if not os.path.exists(BaseDecorator.run_log_file):
            raise FileNotFoundError(f'{BaseDecorator.run_log_file} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. --input-data ì¸ìë¥¼ ì œê³µí•˜ê±°ë‚˜ ë¨¼ì € ì „ì²´ workflowë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.')

        run_log = self._load_log_file()

        # í•´ë‹¹ taskê°€ ì´ë¯¸ ì‹¤í–‰ëœ ì ì´ ìˆìœ¼ë©´ ê·¸ input ì‚¬ìš©
        if task_name in run_log:
            return run_log[task_name]['input']

        # ì‹¤í–‰ëœ ì ì´ ì—†ìœ¼ë©´ ì˜ì¡´ taskë“¤ì˜ outputì„ ë³‘í•©
        target_task = next((task for task in self.depend if task.name == task_name), None)
        if not target_task or not target_task.depend:
            raise ValueError(f'Task "{task_name}"ì˜ ì‹¤í–‰ ë¡œê·¸ê°€ ì—†ê³  ì˜ì¡´ taskë„ ì—†ìŠµë‹ˆë‹¤. --input-dataë¥¼ ì œê³µí•˜ê±°ë‚˜ ë¨¼ì € í•„ìš”í•œ taskë“¤ì„ ì‹¤í–‰í•˜ì„¸ìš”.')

        # ì˜ì¡´ taskë“¤ì˜ output ìˆ˜ì§‘
        merged_input = {}
        missing_deps = []

        for dep_task in target_task.depend:
            if dep_task.name not in run_log:
                missing_deps.append(dep_task.name)
            else:
                # ì˜ì¡´ taskì˜ outputì„ ë³‘í•©
                dep_output = run_log[dep_task.name]['output']
                if isinstance(dep_output, dict):
                    merged_input.update(dep_output)

        if missing_deps:
            raise ValueError(f'Task "{task_name}"ì˜ ì˜ì¡´ taskê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {missing_deps}. ë¨¼ì € í•´ë‹¹ taskë“¤ì„ ì‹¤í–‰í•˜ì„¸ìš”.')

        return merged_input

    def export_airflow(self, output_path=None, schedule_interval=None, start_date=None, default_args=None):
        """
        Airflow DAGë¡œ export

        Args:
            output_path: DAG íŒŒì¼ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: dags/{dag_id}.py)
            schedule_interval: DAG ìŠ¤ì¼€ì¤„ (ê¸°ë³¸: None)
            start_date: DAG ì‹œì‘ ë‚ ì§œ (ê¸°ë³¸: ì˜¤ëŠ˜)
            default_args: DAG default_args

        Returns:
            str: ìƒì„±ëœ DAG íŒŒì¼ ê²½ë¡œ
        """
        exporter = AirflowExporter(self)
        return exporter.export(
            output_path=output_path,
            schedule_interval=schedule_interval,
            start_date=start_date,
            default_args=default_args
        )